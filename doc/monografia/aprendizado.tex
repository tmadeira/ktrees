\chapter{Aprendizado de redes bayesianas}
\label{cap:aprendizado}

Neste capítulo apresentamos o problema de aprender redes bayesianas e descrevemos um método desenvolvido por Nie \emph{et al.} \cite{maua} para resolvê-lo que é baseado na amostragem de \emph{$k$-trees}. Mostramos como a geração uniforme de \emph{$k$-trees} desenvolvida no capítulo \ref{cap:geracao} foi utilizada nesse processo e comparamos os resultados obtidos com os resultados de outros trabalhos.

\section{Motivação}

Aprender uma rede bayesiana se refere ao processo de inferir a estrutura (ou seja, o DAG) dela a partir de dados. Como mostra Chickering \cite{chickering}, este é um problema NP-completo.

O aprendizado de redes bayesianas costuma servir para realizar inferências em situações com incerteza. O artigo de Nie \emph{et al.} \cite{maua} mostra que tais inferências são NP-difíceis até mesmo aproximadamente e todos os algoritmos conhecidos (exatos e comprovadamente bons) têm uma complexidade de pior caso exponencial no \emph{treewidth}.

Além disso, resultados empíricos sugerem que limitar o \emph{treewidth} pode melhorar a performance dos modelos e há evidências de que limitar a \emph{treewidth} da estrutura de uma rede bayesiana não causa perdas significativas na expressividade do modelo para conjuntos de dados reais (também visto em \cite{maua}).

Por isso, estamos interessados em fixar $k$ e aprender redes bayesianas cuja estrutura tem \emph{treewidth} limitada a $k$.

\vspace{2em}

A fim de identificar o ``melhor'' DAG para um determinado conjunto de dados, vamos supôr que há uma função de \emph{score} $s(G)$ que atribui uma pontuação para cada DAG $G$ em tempo constante. Segundo \cite{nie}, as funções de \emph{score} costumam poder ser escritas como a soma de funções de \emph{score} locais, ou seja,

$$s(G) = \sum_{i \in N} s_i(X_{\pi_i}).$$

Para cada variável, sua pontuação só depende do seu conjunto de pais. Ou seja, nosso problema é encontrar $G^*$ tal que

$$G^* = \argmax_{G \in \mathcal{G}_{n,k}} \sum_{i \in N} s_i(\pi_i),$$

onde $\mathcal{G}_{n,k}$ é o conjunto de todos os DAGs de \emph{treewidth} não maiores que $k$.

Mesmo esse problema é NP-difícil, como mostram Korhonen e Parviainen \cite{korhonen}. Entretanto, o artigo \cite{maua} mostra um método aproximado para aprender redes bayesianas com \emph{treewidth} limitado que é baseado em amostrar \emph{$k$-trees} e encontrar DAGs cujo grafo moral é um subgrafo dessas \emph{$k$-trees}.

Tal método funciona com domínios grandes e \emph{treewidth} alto. No artigo é mostrado empiricamente que ele tem um desempenho muito bom numa coleção de conjuntos de dados públicos.

\section{Aprendizado por amostragem de \emph{$k$-trees}}
\label{sec:aprendizado}

A ideia para aprender um DAG por meio da amostragem de \emph{$k$-trees} baseia-se em, para cada \emph{$k$-tree} $T_k$ amostrada, construir uma ordem parcial $\sigma$ dos vértices e fazer com que o DAG $G$ seja consistente com ela e com $T_k$.

Como explica \cite{maua}, \emph{``$\sigma$ restringe as ordenações topológicas válidas para os vértices de $G$''} (de forma que garante que ele não tenha ciclos). Por outro lado, fazer com que $G$ seja subgrafo de $T_k$ garante que seu \emph{treewidth} não exceda $k$.

A continuar. % TODO

\begin{algorithm}[Algoritmo para aprender estrutura]
  \textbf{Entrada:} número de variáveis $n$, a \emph{treewidth} desejada $k$ e uma função de \emph{score} $s_i$ para cada $i \in [0, n)$\\
  \textbf{Saída:} um DAG $G^{\text{melhor}}$

  \begin{enumerate}
    \item Inicializar $G^{\text{melhor}}$ como um grafo vazio com $s(G^{\text{melhor}}) = -\infty$.
    \item Repetir até atingir um determinado número de iterações:
      \begin{enumerate}
        \item Gerar $(Q, S) \in \mathcal{A}^n_k$ conforme seção \ref{sec:geracao};
        \item Decodificar $(Q, S)$ na árvore característica $T$ usando o algoritmo da subseção \ref{subsec:decodificacao};
        \item \emph{TODO: Descrever processo para construir DAG $G$ consistente com $T$} % TODO
        \item Se $\left(\sum_{i \in [0,n)} s_i(\pi^G_{i})\right) = s(G) > s(G^{\text{melhor}})$, atualiza $G^{\text{melhor}} = G$.
      \end{enumerate}
  \end{enumerate}
\end{algorithm}

A continuar. % TODO

\section{Experimentos e resultados}

O algoritmo descrito na seção \ref{sec:aprendizado} foi implementado por João de Santana Brito Junior\footnote{E-mail: joaojr@ime.usp.br} e seu código, que usa o código para gerar \emph{$k$-trees} que foi desenvolvido no capítulo \ref{cap:geracao}, está disponível em \url{https://github.com/britojr/bn}.

A continuar. % TODO
