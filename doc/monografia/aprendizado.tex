\chapter{Aprendizado de redes bayesianas}
\label{cap:aprendizado}

Neste capítulo apresentamos o problema de aprender redes bayesianas e descrevemos um método desenvolvido por Nie \emph{et al.} \cite{maua} para resolvê-lo que é baseado na amostragem de \emph{$k$-trees}. Mostramos como a geração uniforme de \emph{$k$-trees} desenvolvida no capítulo \ref{cap:geracao} foi utilizada nesse processo e comparamos os resultados obtidos com os resultados de outros trabalhos.

\section{Motivação}

Redes bayesianas são modelos probabilísticos gráficos que representam distribuições de probabilidade conjunta e são usados para raciocinar em situações com incerteza.

Formalmente \cite{maua}, seja $N = \{ 1, \cdots, n \}$ e seja $X = \{X_i : i \in N\}$ um conjunto de variáveis aleatórias $X_i$ tomando valores em conjuntos finitos $\mathcal{X}_i$. Uma \textbf{rede bayesiana} é uma tripla $(X, G, \theta)$, onde $G = (V, E)$ é um DAG (que chamamos de \textbf{estrutura} da rede bayesiana) cujos vértices correspondem a variáveis em $X$ e $\theta = \{\theta_i(x_i, x_{\pi_i})\}$ é um conjunto de parâmetros numéricos especificando valores de probabilidade condicional $\theta_i(x_i, x_{\pi_i}) = P(x_i | x_{\pi_i})$ para todo vértice $i \in V$, valor $x_i \in X_i$ e atribuição $x_{\pi_i}$ para os pais $\pi_i$ de $X_i$ (em $G$).

Um exemplo de rede bayesiana é mostrado na figura \ref{fig:bayes}. As arestas codificam nossa intuição sobre a forma como o mundo funciona: as performances do aluno e do supervisor são determinadas independentemente; a nota do TCC (trabalho de conclusão de curso) depende desses dois fatores; a média ponderada do aluno depende apenas da sua performance no curso; a carta de recomendação escrita pelo supervisor sobre o aluno depende da nota do TCC.

\begin{figure}
  \centering
  \begin{tikzpicture}
      [scale=.5,auto=left,every node/.style={draw, ellipse, inner sep = 3pt, minimum width = 0.72cm, align = center}]

      \node[fill=gray!30] (or) at (1,9) {Supervisor};
      \node[fill=gray!30] (al) at (7,9) {Aluno};
      \node[fill=gray!30] (no) at (2,5) {Nota TCC};
      \node[fill=gray!30] (me) at (10,5) {Média ponderada};
      \node[fill=gray!30] (ca) at (0,1) {Carta de recomendação};

      \path (or) edge[-triangle 60] (no)
        (al) edge[-triangle 60] (no)
        (al) edge[-triangle 60] (me)
        (no) edge[-triangle 60] (ca);

      \node[draw=none,inner sep = 0pt,above=of or]
      {
        \begin{tabular}{|c|c|} \hline
          $o_0$ & $o_1$ \\ \hline
          0,5 & 0,5 \\ \hline
        \end{tabular}
      };

      \node[draw=none,inner sep = 0pt,above=of al]
      {
        \begin{tabular}{|c|c|} \hline
          $a_0$ & $a_1$ \\ \hline
          0,5 & 0,5 \\ \hline
        \end{tabular}
      };

      \node[draw=none,inner sep = 0pt,below=of me]
      {
        \begin{tabular}{|c|c|c|} \hline
                & $m_0$ & $m_1$ \\ \hline
          $a_0$ & 0,8 & 0,2 \\ \hline
          $a_1$ & 0,2 & 0,8 \\ \hline
        \end{tabular}
      };

      \node[draw=none,inner sep = 0pt,left=of no]
      {
        \begin{tabular}{|c|c|c|} \hline
                & $n_0$ & $n_1$ \\ \hline
          $a_0,o_0$ & 0,9 & 0,1 \\ \hline
          $a_0,o_1$ & 0,7 & 0,3 \\ \hline
          $a_1,o_0$ & 0,5 & 0,5 \\ \hline
          $a_1,o_1$ & 0,1 & 0,9 \\ \hline
        \end{tabular}
      };

      \node[draw=none,inner sep = 0pt,below=of ca]
      {
        \begin{tabular}{|c|c|c|} \hline
                & $c_0$ & $c_1$ \\ \hline
          $n_0$ & 0,9 & 0,1 \\ \hline
          $n_1$ & 0,1 & 0,9 \\ \hline
        \end{tabular}
      };
  \end{tikzpicture}

  \caption{Exemplo de rede bayesiana com distribuições de probabilidade condicional.}
  \label{fig:bayes}
\end{figure}

Redes bayesianas são geralmente usadas para fazer inferências como computar a probabilidade de alguma variável depois que alguma evidência é observada. Por exemplo, uma rede bayesiana pode representar as relações de probabilidade entre doenças e sintomas. Observados alguns sintomas, a rede pode ser usada para computar a probabilidade da presença das doenças.

Para dar um exemplo mais concreto, com a rede bayesiana mostrada na figura \ref{fig:bayes} pode-se modificar a crença sobre a performance do aluno ou do supervisor com base na nota do TCC.

\vspace{2em}

Aprender uma rede bayesiana se refere ao processo de inferir a estrutura (ou seja, o DAG) dela a partir de dados. Como mostra Chickering \cite{chickering}, este é um problema NP-completo.

O aprendizado de redes bayesianas costuma servir para realizar inferências em situações com incerteza. O artigo de Nie \emph{et al.} \cite{maua} mostra que tais inferências são NP-difíceis até mesmo aproximadamente e todos os algoritmos conhecidos (exatos e comprovadamente bons) têm uma complexidade de pior caso exponencial no \emph{treewidth}.

Além disso, resultados empíricos sugerem que limitar o \emph{treewidth} pode melhorar a performance dos modelos e há evidências de que limitar a \emph{treewidth} da estrutura de uma rede bayesiana não causa perdas significativas na expressividade do modelo para conjuntos de dados reais (também visto em \cite{maua}).

Por isso, estamos interessados em fixar $k$ e aprender redes bayesianas cuja estrutura tem \emph{treewidth} limitado a $k$.

\vspace{2em}

A fim de identificar o ``melhor'' DAG para um determinado conjunto de dados, vamos supôr que há uma função de \emph{score} $s(G)$ que atribui uma pontuação para cada DAG $G$ em tempo constante. Segundo \cite{nie}, as funções de \emph{score} costumam poder ser escritas como a soma de funções de \emph{score} locais, ou seja,

$$s(G) = \sum_{i \in N} s_i(X_{\pi_i}).$$

Para cada variável, sua pontuação só depende do seu conjunto de pais. Ou seja, nosso problema é encontrar $G^*$ tal que

$$G^* = \argmax_{G \in \mathcal{G}_{n,k}} \sum_{i \in N} s_i(\pi_i),$$

onde $\mathcal{G}_{n,k}$ é o conjunto de todos os DAGs de \emph{treewidth} não maiores que $k$.

Mesmo esse problema é NP-difícil, como mostram Korhonen e Parviainen \cite{korhonen}. Entretanto, o artigo \cite{maua} mostra um método aproximado para aprender redes bayesianas com \emph{treewidth} limitado que é baseado em amostrar \emph{$k$-trees} e encontrar DAGs cujo grafo moral é um subgrafo dessas \emph{$k$-trees}.

Tal método funciona com domínios grandes e \emph{treewidth} alto. No artigo é mostrado empiricamente que ele tem um desempenho muito bom numa coleção de conjuntos de dados públicos.

\section{Aprendizado por amostragem de \emph{$k$-trees}}
\label{sec:aprendizado}

A ideia para aprender um DAG por meio da amostragem de \emph{$k$-trees} baseia-se em, para cada \emph{$k$-tree} $T_k$ amostrada, construir uma ordem parcial $\sigma$ dos vértices e fazer com que o DAG $G$ seja consistente com ela e com $T_k$.

Como explica \cite{maua}, \emph{``$\sigma$ restringe as ordenações topológicas válidas para os vértices de $G$''} de forma que garante que ele não tenha ciclos. Por outro lado, fazer com que $G$ seja subgrafo de $T_k$ garante que seu \emph{treewidth} não exceda $k$.

Construímos a ordem parcial $\sigma$ a partir do enraizamento da \emph{$k$-tree} num $k$-clique qualquer. Primeiro, sorteamos uniformemente a ordem dos vértices do $k$-clique raiz. A partir dele, visitamos os vértices da árvore na ordem da busca em largura. Cada vértice que visitamos é inserido num lugar aleatório da ordem, que é definido de forma uniforme. Determinado esse lugar, selecionamos os melhores pais para ele dentre os vértices adjacentes predecessores já visitados e atualizamos os melhores pais dos vértices adjacentes.

O passo 3 do algoritmo de decodificação de \emph{Dandelion Code} em \emph{$k$-tree} apresentado na subseção \ref{subsec:decodificacao} percorre a árvore exatamente como gostaríamos para construir uma \emph{$k$-tree} de Rényi a partir de uma árvore característica, o que sugere que construamos o DAG diretamente a partir da árvore característica.

O algoritmo de aprendizado fica como segue:

\begin{algorithm}[Algoritmo para aprender estrutura]
  \textbf{Entrada:} número de variáveis $n$, a \emph{treewidth} desejada $k$ e uma função de \emph{score} $s_i$ para cada $i \in [0, n)$\\
  \textbf{Saída:} um DAG $G^{\text{melhor}}$

  \begin{enumerate}
    \item Inicializar $G^{\text{melhor}}$ como um grafo vazio com $s(G^{\text{melhor}}) = -\infty$.
    \item Repetir até atingir um determinado número de iterações:
      \begin{enumerate}
        \item Gerar $(Q, S) \in \mathcal{A}^n_k$ conforme seção \ref{sec:geracao};
        \item Decodificar $(Q, S)$ na árvore característica $T$ usando o algoritmo da subseção \ref{subsec:decodificacao};
        \item Determinar uniformemente uma ordem pros vértices do $k$-clique raiz (ou seja, os vértices correspondentes a $\phi^{-1}(v)$ para os vértices $v \in \{n-k+1, n-k+2, \cdots, n\}$ correspondentes a raiz da \emph{$k$-tree} de Rényi) e usar a função de \emph{score} para calcular os melhores pais para cada um deles entre os vértices que os precedem;
        \item Percorrer $T$ como no passo 3 do algoritmo \ref{subsec:decodificacao} a partir dos vértices ligados ao $k$-clique raiz: para cada $v$, é sorteado um lugar para ele em $\sigma$ e selecionado seu melhor conjunto de pais dentre os vértices predecessores (em $\sigma$) adjacentes já visitados, assim como são atualizados os melhores pais dos vértices sucessores (em $\sigma$) adjacentes já visitados;
        \item Se $\left(\sum_{i \in [0,n)} s_i(\pi^G_{i})\right) = s(G) > s(G^{\text{melhor}})$, atualiza $G^{\text{melhor}} = G$.
      \end{enumerate}
  \end{enumerate}
\end{algorithm}

\section{Experimentos e resultados}

O algoritmo descrito na seção \ref{sec:aprendizado} foi implementado por João de Santana Brito Junior\footnote{Aluno de mestrado orientado pelo Prof. Dr. Denis Deratani Mauá. E-mail: \href{mailto:joaojr@ime.usp.br}{joaojr@ime.usp.br}} e seu código, que faz uso da biblioteca para gerar \emph{$k$-trees} que foi desenvolvida no capítulo \ref{cap:geracao}, está disponível em \url{https://github.com/britojr/bn}.

Usamos essa implementação para testar o aprendizado por amostragem uniforme de \emph{$k$-trees} com cinco conjuntos de dados reais contendo de 64 a 1556 variáveis. A tabela \ref{tab:conjuntos} descreve as características de cada um deles.

\begin{table}[h]
  \centering
  \begin{tabular}{l c c} \hline
    \emph{Dataset} & $n$ & $N$ \\ \hline
    kdd & 64 & 234954 \\
    tretail & 135 & 29387 \\
    cr52 & 889 & 9100 \\
    bbc & 1058 & 2225 \\
    ad & 1556 & 3279 \\ \hline
  \end{tabular}

  \caption{Características dos conjuntos de dados utilizados; $n$ é o número de variáveis e $N$ é o número de instâncias.}
  \label{tab:conjuntos}
\end{table}

Rodamos cada experimento 30 vezes num computador com processador Intel Core i5 2.6GHz e 8GB RAM.

% TODO: A tabela \ref{tab:tempos} mostra quanto tempo levou em média o aprendizado com diferentes parâmetros $k$ e 1000 iterações.
%
%\begin{table}
%  \centering
%  \begin{tabular}{|c|c|c|c|} \hline
%    & $k = 4$ & $k = 8$ & $k = 10$ \\ \hline
%    kdd & & & \\
%    tretail & & & \\
%    cr52 & & & \\
%    bbc & & & \\
%    ad & & & \\ \hline
%  \end{tabular}
%
%  \caption{Descrição da tabela}
%  \label{tab:tempos}
%\end{table}

A tabela \ref{tab:comparacao} mostra a média dos \emph{scores} normalizados obtidos nos nossos experimentos e os compara com os resultados obtidos no artigo de Perez e Mauá \cite{perez} para o \emph{Acyclic Selection Order-Based Search (ASOBS)} com \emph{Best First-Based initialization heuristic (BFT)}, uma abordagem de aprendizado que tem resultados comparáveis ao estado da arte para domínios grandes.

\begin{table}[h]
  \centering

  {\footnotesize
    \begin{tabular}{l c c c c c c} \hline
      & \multicolumn{2}{c}{$k = 4$} & \multicolumn{2}{c}{$k = 10$} & \multicolumn{2}{c}{Perez e Mauá} \\ \hline
      \emph{Dataset} & Máx. & Média & Máx. & Média & Máx. & Média \\ \hline
      kdd & 0,0755 & 0,0691 $\pm$ 0,0022 & 0,1037 & 0,1007 $\pm$ 0,0016 & 0,1468 & 0,1447 $\pm$ 0,0007 \\
      tretail & 0,0188 & 0,0156 $\pm$ 0,0019 & 0,0289 & 0,0249 $\pm$ 0,0017 & 0,0447 & 0,0444 $\pm$ 0,0002 \\
      cr52 & 0,0203 & 0,0171 $\pm$ 0,0011 & 0,0439 & 0,0333 $\pm$ 0,0028 & 0,1617 & 0,1598 $\pm$ 0,0010 \\
      bbc & 0,0059 & 0,0054 $\pm$ 0,0002 & 0,0128 & 0,0102 $\pm$ 0,0005 & 0,0777 & 0,0770 $\pm$ 0,0004 \\
      ad & 0,0448 & 0,0400 $\pm$ 0,0023 & 0,0905 & 0,0781 $\pm$ 0,0045 & 0,7114 & 0,7089 $\pm$ 0,0013 \\ \hline
    \end{tabular}
  }

  \caption{Performance do aprendizado de redes bayesianas por amostragem de \emph{$k$-trees} e comparação dele com os resultados obtidos por Perez e Mauá.}
  \label{tab:comparacao}
\end{table}

\emph{A analisar.} % TODO
